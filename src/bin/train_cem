import argparse
import time
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

from simpledt.collect import collect_rollout
from simpledt.models.mlp_policy import MLPPolicy
from simpledt.models.transformer_policy import TransformerPolicy
from simpledt.replay_buffer import ReplayBuffer
from simpledt.cem_optimizer import CEMOptimizer, get_best_n_rollouts
from simpledt.tb import log_dict_to_tensorboard


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", type=str, default="Pendulum-v1")
    parser.add_argument("--hidden-size", type=int, default=32)
    parser.add_argument("--num-layers", type=int, default=2)
    parser.add_argument("--nhead", type=int, default=2)
    parser.add_argument("--dim-feedforward", type=int, default=32)
    parser.add_argument("--max-steps", type=int, default=200)
    parser.add_argument("--num-rollouts-per-epoch", type=int, default=200)
    parser.add_argument("--num-eval-rollouts-per-epoch", type=int, default=20)
    parser.add_argument("--num-epochs", type=int, default=1000)
    parser.add_argument("--num-train-ops-per-epoch", type=int, default=200)
    parser.add_argument("--learning-rate", type=float, default=3e-4)
    parser.add_argument("--batch-size", type=int, default=20)
    parser.add_argument("--num-best", type=int, default=20)
    parser.add_argument("--policy-type", type=str, default="transformer")
    parser.add_argument("--tb", type=str, default=None)
    parser.add_argument("--device", type=str, default="cpu")
    return parser.parse_args()


def collect_rollouts(
    num_rollouts,
    env,
    policy,
    max_steps,
    exploration,
    replay_buffer,
    print_each=10,
    print_prefix=''
):
    t1 = time.time()
    for i in range(num_rollouts):
        rollout = collect_rollout(
            env=env,
            policy=policy,
            max_steps=max_steps,
            exploration=exploration
        )
        rollout.observations = {"observation": rollout.observations}
        if i % print_each == 0:
            t2 = time.time()
            print(f'--- {print_prefix} rollout {i} {t2 - t1} reward {rollout.rewards.sum()}')
            t1 = t2
        replay_buffer.add_rollout(rollout)


def main():
    # Parse the command-line arguments
    args = parse_args()

    # Create the environment and the policy
    env = gym.make(args.env)
    env_render = gym.make(args.env, render_mode="human")
    obs_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    if args.policy_type == "mlp":
        policy = MLPPolicy(
            obs_size,
            action_size,
            hidden_size=args.hidden_size,
            device=args.device,
        )
    elif args.policy_type == "transformer":
        policy = TransformerPolicy(
            obs_size,
            action_size,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers,
            nhead=args.nhead,
            dim_feedforward=args.dim_feedforward,
            output_seq_len=args.max_steps,
            batch_first=True,
            device=args.device,
        )
    else:
        raise RuntimeError(f"policy type {args.policy_type} is not supported")

    # Create the replay buffer
    observation_shape = {"observation": (obs_size,)}
    action_shape = (action_size,)
    info_shape = {}

    replay_buffer = ReplayBuffer(
        max_size=args.num_rollouts_per_epoch,
        rollout_len=args.max_steps,
        observation_shape=observation_shape,
        action_shape=action_shape,
        info_shape=info_shape,
    )

    best_replay_buffer = ReplayBuffer(
        max_size=args.num_best,
        rollout_len=args.max_steps,
        observation_shape=observation_shape,
        action_shape=action_shape,
        info_shape=info_shape,
    )

    eval_replay_buffer = ReplayBuffer(
        max_size=args.num_eval_rollouts_per_epoch,
        rollout_len=args.max_steps,
        observation_shape=observation_shape,
        action_shape=action_shape,
        info_shape=info_shape,
    )

    cem_optimizer = CEMOptimizer(
        policy=policy,
        optimizer=optim.Adam(params=policy.parameters(), lr=args.learning_rate),
        criterion=nn.MSELoss(),
        device=args.device,
    )

    torch.set_printoptions(sci_mode=False)
    tb = SummaryWriter(args.tb) if args.tb else None
    train_step = 0

    # Collect rollouts and put them into the replay buffer
    for epoch in range(args.num_epochs):
        if epoch % 10 == 0:
            collect_rollouts(
                num_rollouts=args.num_eval_rollouts_per_epoch,
                env=env,
                policy=policy,
                max_steps=args.max_steps,
                exploration=0,
                replay_buffer=eval_replay_buffer,
                print_prefix='eval',
            )
            eval_rewards_t = torch.sort(eval_replay_buffer.rewards[..., 0].sum(-1), descending=True)[0]
            mean_eval_rewards = torch.mean(eval_rewards_t).item()
            log_dict_to_tensorboard({'mean_reward': {'eval_mean': mean_eval_rewards}}, epoch, tb)
            print(f'--- eval rewards {mean_eval_rewards} {eval_rewards_t}')

        collect_rollouts(
            num_rollouts=args.num_rollouts_per_epoch,
            env=env,
            policy=policy,
            max_steps=args.max_steps,
            exploration=0.4,
            replay_buffer=replay_buffer,
            print_prefix=f'epoch {epoch}',
        )

        all_rollouts = replay_buffer.get_content()
        best_n = get_best_n_rollouts(args.num_best, all_rollouts)
        best_replay_buffer.set_content(best_n)
        mean_best = best_n.rewards.sum(1).mean()
        mean_all = all_rollouts.rewards.sum(1).mean()
        log_dict_to_tensorboard({
            'mean_reward': {
                'train_best_mean': mean_best,
                'train_all_mean': mean_all,
            }
        }, epoch, tb)
        print(
            f'--- rewards {mean_best} / {mean_all} '
            f'{best_n.rewards.sum(1)[..., 0]}'
        )

        # need to do something with edge case in reward to go at the end of seqs
        for _ in range(args.num_train_ops_per_epoch):
            batch = best_replay_buffer.sample(args.batch_size)
            batch.observations = batch.observations["observation"][:, :-1]
            train_info = cem_optimizer.train_on_batch(batch)
            if train_step % 25 == 0:
                eval_batch = eval_replay_buffer.sample(args.batch_size)
                eval_batch.observations = eval_batch.observations["observation"][:, :-1]
                valid_info = cem_optimizer.validate_on_batch(eval_batch)
                if tb:
                    log_dict_to_tensorboard(train_info, train_step, tb)
                    log_dict_to_tensorboard(valid_info, train_step, tb)
                print(f'--- epoch {epoch} {train_step} info {train_info} {valid_info}')
            train_step += 1


if __name__ == "__main__":
    main()
