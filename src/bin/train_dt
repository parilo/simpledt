import argparse
import gymnasium as gym
import torch.nn as nn
import torch.optim as optim

from simpledt.collect import collect_rollout
from simpledt.models.dtpolicy2 import DTPolicy
from simpledt.replay_buffer import ReplayBuffer
from simpledt.simple_dt_optimizer import SimpleDTOptimizer


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", type=str, default="Pendulum-v1")
    parser.add_argument("--hidden-size", type=int, default=64)
    parser.add_argument("--num-layers", type=int, default=2)
    parser.add_argument("--nhead", type=int, default=16)
    parser.add_argument("--dim-feedforward", type=int, default=256)
    parser.add_argument("--max-steps", type=int, default=100)
    parser.add_argument("--num-rollouts-per-epoch", type=int, default=10)
    parser.add_argument("--num-epochs", type=int, default=1)
    parser.add_argument("--num-train-ops-per-epoch", type=int, default=1)
    parser.add_argument("--discount-factor", type=float, default=0.99)
    parser.add_argument("--learning-rate", type=float, default=3e-4)
    parser.add_argument("--batch-size", type=int, default=64)
    parser.add_argument("--device", type=str, default="cpu")
    return parser.parse_args()


def main():
    # Parse the command-line arguments
    args = parse_args()

    # Create the environment and the policy
    env = gym.make(args.env)
    env_render = gym.make(args.env, render_mode="human")
    obs_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    policy = DTPolicy(
        obs_size,
        action_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        nhead=args.nhead,
        dim_feedforward=args.dim_feedforward,
        output_seq_len=args.max_steps,
        batch_first=True,
        device=args.device,
    )

    # Create the replay buffer
    observation_shape = {"observation": (obs_size,)}
    action_shape = (action_size,)
    info_shape = {}
    replay_buffer = ReplayBuffer(
        max_size=args.num_rollouts_per_epoch,
        rollout_len=args.max_steps,
        observation_shape=observation_shape,
        action_shape=action_shape,
        info_shape=info_shape,
    )

    simple_dt_optimizer = SimpleDTOptimizer(
        policy=policy,
        optimizer=optim.Adam(params=policy.parameters(), lr=args.learning_rate),
        criterion=nn.MSELoss(),
        device=args.device,
        discount_factor=args.discount_factor,
    )

    # Collect rollouts and put them into the replay buffer
    for epoch in range(args.num_epochs):
        for i in range(args.num_rollouts_per_epoch):
            # used_env = env_render if i == 0 else env
            used_env = env
            rollout = collect_rollout(used_env, policy, args.max_steps)
            rollout.observations = {"observation": rollout.observations}
            if i % 10 == 0:
                print(f'--- rollout {i} reward {rollout.rewards.sum()}')
            replay_buffer.add_rollout(rollout)

        # need to do something with edge case in reward to go at the end of seqs
        for train_op_i in range(args.num_train_ops_per_epoch):
            batch = replay_buffer.sample(args.batch_size)
            batch.observations = batch.observations["observation"][:, :-1]
            train_info = simple_dt_optimizer.train_on_batch(batch, train_op_i == 0)
            if train_op_i % 100 == 0:
                print(f'--- epoch {epoch} {train_op_i} info {train_info}')


if __name__ == "__main__":
    main()
